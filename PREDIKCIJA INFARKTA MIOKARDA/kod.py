# -*- coding: utf-8 -*-
"""Diplomski_v1_srce.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HmX0Phn-uaw4dMSXlkTvBjDRsNHf-SPG
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# ===================================================================
# PRVA ĆELIJA: Povezivanje, KOPIRANJE PODATAKA i otpakivanje
# ===================================================================

from google.colab import drive
import os

# 1. Poveži se sa Google Drive-om
drive.mount('/content/drive')

# 2. Definiši putanje
DRIVE_BASE_PATH = '/content/drive/MyDrive/Diplomski/' # Putanja do podataka na tvom Drive-u
LOCAL_DATA_PATH = '/content/Podaci/' # Napravićemo novi, lokalni folder u Colab-u

# 3. Napravi lokalni folder
os.makedirs(LOCAL_DATA_PATH, exist_ok=True)

print("Počinje kopiranje podataka sa Google Drive-a na lokalni disk Colab-a...")
print("Ovo može potrajati nekoliko minuta, ali će drastično ubrzati ceo proces.")

# 4. Kopiraj SVE potrebne fajlove sa Drive-a na lokalni disk
# Koristimo !cp komandu (copy). Ovo je ključni korak!
!cp "{DRIVE_BASE_PATH}ptbxl_database.csv" "{LOCAL_DATA_PATH}"
!cp "{DRIVE_BASE_PATH}scp_statements.csv" "{LOCAL_DATA_PATH}"
!cp "{DRIVE_BASE_PATH}records100.rar" "{LOCAL_DATA_PATH}"
!cp "{DRIVE_BASE_PATH}records500.rar" "{LOCAL_DATA_PATH}"

print("\nKopiranje završeno. Počinje lokalno otpakivanje...")

# 5. Instaliraj unrar
!apt-get install -y unrar > /dev/null # "> /dev/null" sakriva dugačak izlaz

# 6. Otpakuj fajlove koji se SADA NALAZE LOKALNO
!unrar x -o+ "{LOCAL_DATA_PATH}records100.rar" "{LOCAL_DATA_PATH}"
!unrar x -o+ "{LOCAL_DATA_PATH}records500.rar" "{LOCAL_DATA_PATH}"

print("\nSve je spremno! Podaci su sada na brzom lokalnom disku.")

# Importovanje neophodnih biblioteka za rad
import pandas as pd
import numpy as np
import wfdb
import os
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tqdm import tqdm
import warnings

from scipy.signal import butter, filtfilt, iirnotch, savgol_filter
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import LabelBinarizer, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc

from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling1D
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Resavanje warninga
warnings.filterwarnings("ignore")

# Konfiguracija ključnih parametara
SAMPLING_RATE = 100 # Moguće vrednosti su 100 ili 500 jer imam takve frekvencije
FILTER_TYPE = 'bandpass' # Moguće vrednosti: 'bandpass', 'notch', 'savgol'
SELECTED_MODEL = 'simple' # 'simple' ili 'advanced'
MODEL_TYPE = '2D'  # Moguće vrednosti: '1D', '2D'
BASE_PATH = '/content/Podaci/'

# Uključivanje/isključivanje K-Fold unakrsne validacije
USE_KFOLD = True   # Moguće vrednosti su True ili False
K_FOLDS = 5        # Broj fold-ova ako je USE_KFOLD True (najčešće vrednosti 5 ili 10)

# --- Učitavanje i Predobrada (kao i pre) ---
def load_data(sampling_rate, base_path):
    print(f"Učitavanje podataka sa frekvencijom od {sampling_rate} Hz...")
    # Kreiranje putanji do csv fajlova iz podataka
    db_path = os.path.join(base_path, 'ptbxl_database.csv')
    scp_path = os.path.join(base_path, 'scp_statements.csv')
    # Učitavanje glavnog fajla i postavljanje jedinstvenog identifikatora za svaki red
    data = pd.read_csv(db_path, index_col='ecg_id')
    # Kolona scp_codes koja će mi biti od značaja je sačuvana kao string i pretvaram je u pajton rečnik
    data.scp_codes = data.scp_codes.apply(lambda x: eval(x))
    # Učitavanje fajla koji sadrži opise dijiagnostičkih kodova
    scp_statements = pd.read_csv(scp_path, index_col=0)
    # Filtriranje i zadržavanje samo onih kodova koji predstavljaju potvrđene dijagnoze
    scp_statements = scp_statements[scp_statements.diagnostic == 1]
    # Pomoćna funkcija za jednostavniji pristup dijagnozama
    def aggregate_diagnostic(y_dic):
      # Prolazim kroz sve dijagnostičke kodove za jednog pacijenta
        for key in y_dic.keys():
            # Ako pronađem kod koji sadrži MI odnosno infarkt miokarda njega označavam kao MI odmah
            if key in scp_statements.index and scp_statements.loc[key].diagnostic_class == 'MI': return 'MI'
        # Ako nisam pronašao MI nakon provere svih kodova označavam pacijenta kao Normal
        return 'Normal'
    # Kreirao sam novu pomoćnu kolonu primenom funkcije iznad
    data['diagnostic_superclass'] = data.scp_codes.apply(aggregate_diagnostic)
    # Filtriram ceo dataset kako bih zadržao samo pacijente iz dve klase koje mene interesuju
    data = data[data.diagnostic_superclass.isin(['MI', 'Normal'])]
    # Dalje sam radio balansiranje dataseta kako bih sprečio da model bude pristrasniji brojnijoj klasi
    # Prebrojao sam koliko pacijenata imam u svakoj klasi i uzeo manji od ta dva broja i to mi predstavlja broj uzoraka po klasi
    n_samples = min(data.diagnostic_superclass.value_counts())
    # Nasumično biram broj uzoraka pacijenata iz MI klase i iz Normal klase
    mi_data = data[data.diagnostic_superclass == 'MI'].sample(n_samples, random_state=42)
    normal_data = data[data.diagnostic_superclass == 'Normal'].sample(n_samples, random_state=42)
    # Spojio sam ove dve balansirane grupe u jedan konačni balansirani dataset
    data = pd.concat([mi_data, normal_data])
    # Inicijalizovao sam prazne liste za smeštanje sirovih signala i njihovih labela
    X, y = [], []
    # Petlja za prolazak kroz svaki red dataseta, tqdm koristim za statusnu traku
    for ecg_id, row in tqdm(data.iterrows(), total=data.shape[0], desc="Učitavanje EKG signala"):
        # Na osnovu sampling_rate-a biram putanju do fajla koji mi sadrži signale
        file_path = os.path.join(base_path, row[f'filename_{"lr" if sampling_rate == 100 else "hr"}'])
        # Ukoliko slučajno ne postoji neki fajl u folderu try-catch blok ispiše upozorenje i nastavi dalje izvršavanje
        try:
            # Pomoću wfdb biblioteke učitavam sirovi EKG signal i meta-podatke
            signal, meta = wfdb.rdsamp(file_path)
            # Dodao sam učitani signal u listu X
            X.append(signal)
            # Isto tako odgovarajuću labelu ("Mi" ili "Normal") u listu Y
            y.append(row['diagnostic_superclass'])
        except FileNotFoundError: print(f"Upozorenje: Fajl {file_path} nije pronađen.")
    print("Učitavanje podataka završeno.")
    # Na kraju konvertujem Python liste u Numpy nizove
    return np.array(X), np.array(y)

# Funkcija za preprocesiranje podataka primenom odgovarajućeg filtera
def preprocess_signals(X_raw, fs, filter_type='bandpass'):
    print(f"Primena '{filter_type}' filtera...")
    # Na samom početku pravim kopiju ulaznih podataka koja mi daje sigurnost da sirovi podaci ostaju nepromenjeni i da ih kasnije mogu uporediti sa obrađenim podaccima
    X_filtered = np.copy(X_raw)
    # Dizajniranje filtera
    if filter_type == 'notch':
        # Dizajiram Notch filter da ukloni smetnju od 50Hz, Q faktor (30) određuje oštrinu filtera
        b, a = iirnotch(50, 30, fs)
    elif filter_type == 'bandpass':
        # Definisanje graničnih frekvencija filtera: lowcut uklanja veoma spore promene u signalu, highcut uklanja šum visoke frekvencije
        lowcut, highcut = 0.5, 45.0
        # Računam Nikvistovu frekveniju koja mi predstavlja polovinu frekvencije uzorkovanja
        nyquist = 0.5 * fs
        # Koristim Batervortov filter koji mi predstavlja standard za EKG obradu jer ima ravno propusno područje i ne unosi distorziju
        b, a = butter(2, [lowcut / nyquist, highcut / nyquist], btype='band')
    # NAPOMENA - Za Savitzky-Golay filter nema pre-dizajniranja, on se primenjuje direktno
    # Petlja za statusnu traku
    for i in tqdm(range(X_raw.shape[0]), desc=f"Primena '{filter_type}' filtera"):
        # Unutrašnja petlja koja prolazi kroz svaki od 12 kanala
        for j in range(X_raw.shape[2]):
            # Primena filtera, bandpass i notch filter se primenjuju na filtfilt
            if filter_type in ['notch', 'bandpass']:
                X_filtered[i, :, j] = filtfilt(b, a, X_raw[i, :, j])
            # Savgol filter se primenjuje direktno, sa standardnim vrednostima
            elif filter_type == 'savgol':
                X_filtered[i, :, j] = savgol_filter(X_raw[i, :, j], window_length=11, polyorder=2)

    # Dalje je rađena standardizacija signala
    print("Standardizacija signala...")
    scaler = StandardScaler()
    X_scaled = np.copy(X_filtered)
    # Skaliramo svaki EKG signal individualno
    for i in tqdm(range(X_filtered.shape[0]), desc="Standardizacija"):
        # fit_transform izračunava srednju vrednost i standardnu devijaciju za podatke i onda koristi te izračunate vrednosti da skalira podatke
        X_scaled[i, :, :] = scaler.fit_transform(X_filtered[i, :, :])

    print("Predobrada završena.")
    # Vraćam potpuno obrađene podatke
    return X_scaled

# 2D Modeli
# Obe naredne funkcije koriste Keras Functional API, napredniji i fleksibilniji način definisanja modela
def build_simple_2d_cnn(input_shape):
    # Definišem oblik ulaznih podataka, input_shape će mi biti (1000, 12, 1) za 100Hz signale
    inputs = Input(shape=input_shape)
    # PRVI KONVOLUCIONI BLOK
    ''' Ovde primenjujem 32 različita filtera veličine 3x3 na ulaznu sliku
        Svaki filter je obučen da prepozna mali lokalni obrazac (npr. ivicu, nagib, mali pik)
        Pomoću ReLU aktivacione funkcije unešena je linearnost kako bi mreža mogla da uči kompleksne obrasce, ona jednostavno menja sve vrednosti u 0
        padding="same" osiguraa da izlazna mapa karakteristika ima istu visinu i širinu kao ulazna
    '''
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
    '''
        MaxPooling2D smanjuje dimenzionalnost (visinu i širinu) mape karakteristika
        Gleda u prozore veličine 2x2 i zadržava najveću (najaktivniju) vrednost
        Ovo čini model otpornijim na male translacije u signalu i smanjuje broj parametara
    '''
    x = MaxPooling2D((2, 2))(x)

    # DRUGI KONVOLUCIONI BLOK
    '''
        Povećavam broj filtera na 64, prvi slojevi uče jednostavne obrasce a dublji slojevi te jednostavne obrasce kombinuju da bi naučili kompleksnije (npr. ceo QRS kompleks)
        name = 'last_conv_layer' dajem specifično ime ovom sloju da bi Grad-CAM tehnika kasnije znala gde da gleda
    '''
    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='last_conv_layer')(x)
    x = MaxPooling2D((2, 2))(x)
    # KLASIFIKACIJA
    '''
        Nakon konvolucionih slojeva sam dobio mapu 2D karakteristika
        Flatten sloj je razvija u dugačak jednodimenzionalni vektor.
        Time sam pripremio podatke za ulaz u standardne potpuno povezane slojeve
    '''
    x = Flatten()(x)
    '''
        Dense Layer (potpuno povezani sloj) je klasičan sloj neuronske mreže gde je svaki neuron povezan sa svim neuronima iz prethodng sloja
        Sadrži 128 neurona i služi da nauči kombinacije karakteristika otkrivenih u konvolucionim slojevima
    '''
    x = Dense(128, activation='relu')(x)
    '''
        Output Layer (Izlazni sloj) je finalni sloj koji donosi odluku
        Ima tačno 2 neurona, po jedan za svaku klasu (MI i Normal)
        Softmax funkcija pretvara izlazne vrednosti neurona u verovatnoće
        Zbir verovatnoća za sve klase će uvek biti 1, neuron sa većom verovatnoćom predstavlja predikciju modela
    '''
    outputs = Dense(2, activation='softmax')(x)

    # Kreiram finalni model tako što kažem Keras-u koj je ulazni, a koji izlazni sloj
    model = Model(inputs=inputs, outputs=outputs)
    '''
        Konfigurišem proces treniranja
        Adam je najčešći i najpoznatiji optimizacioni algoritam i on radi tako što prilagođava težine modela
        Funkcija gubitka (categorical_crossentropy) je izabrana jer se koristi za višeklasnu klasifikaciju
        Ona meri koliko je predikcija modela zapravo dobra u odnosu na stvarnu vrednost
        accuracy je metrika koju pratim tokom treninga
    '''
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

def build_advanced_2d_cnn(input_shape):
    # Slično kao i za simple funkciju definišem oblik ulaznih podataka
    inputs = Input(shape=input_shape)

    # Sada počinjem sa 64 filtera a zatim ih povećavam na 128 kako bi model učio kompleksnije obrasce
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
    # TEHNIKE REGULARIZACIJE
    # Ove tehnike sam uveo kako bih pomogao modelu da se bori protiv overfittinga i da stabilizujem trening
    # Ovaj slij normalizuje izlaz iz prethodnog sloja (srednja vrednost 0, standardna devijacija 1) za svaki batch podataka što ubrzava i stabilizuje proces učenja
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    # Sve isto i u drugom bloku
    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='last_conv_layer')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)
    x = Flatten()(x)
    # Povećao sam kapacitet i u Dense sloju
    x = Dense(256, activation='relu')(x)
    x = BatchNormalization()(x)
    '''
        Druga tehnika regularizcije mi predstavlja dropout
        Tokom treninga ovaj sloj nasumično gasi 50% neurona iz prethodnog sloja u svakoj iteraciji
        To primorava mrežu da ne postane zavisna od bilo kog prethodnog neurona i da uči robusnije, generalizovanije karakteristike
    '''
    x = Dropout(0.5)(x)
    outputs = Dense(2, activation='softmax')(x)

    # Kreiram finalni model, korišten Adam sa specificiranom manjom stopom učenja koja dovodi do boljeg pronalaženja minimuma funkcije
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                  loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# 1D Modeli
# Obe naredne funkcije koriste Sequential API jer za njih nije potrebna implementacija Grad-CAM tehnike
def build_simple_1d_cnn(input_shape):
    '''
        Definišem oblik ulaznih podataka koji će za 1D CNN biti (vreme, kanali)
        Ostali delovi do Global Average Pooling 1D su isti
        Ovaj deo radi sledeće: umesto da ispravi (Flatten) mapi karakteristika u ogroman vektor, on za svaki od 64 filtera izračunava prosečnu vrednost aktivacije duže cele vremenske ose
        Rezultat toga je fiksni vektor od 64 broja bez obzira na dužinu ulazne sekvence
        To smanjuje broj parametara i rizik od overfittinga
        Sve ostalo je isto kao i za 2D model
    '''
    model = Sequential([Input(shape=input_shape), Conv1D(32, 5, activation='relu', padding='same'), MaxPooling1D(2), Conv1D(64, 5, activation='relu', padding='same'), GlobalAveragePooling1D(), Dense(128, activation='relu'), Dense(2, activation='softmax')])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Ova funkcija što se tiče logike je ista kao i za 2D model
def build_advanced_1d_cnn(input_shape):
    model = Sequential([Input(shape=input_shape), Conv1D(64, 5, activation='relu', padding='same'), BatchNormalization(), MaxPooling1D(2), Conv1D(128, 5, activation='relu', padding='same'), BatchNormalization(), GlobalAveragePooling1D(), Dense(256, activation='relu'), BatchNormalization(), Dropout(0.5), Dense(2, activation='softmax')])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Grad-CAM tehnika mi pomaže da na osnovu mape važnosti vidim na koje delove ulazne slike (EKG signala) je konvoluciona mreža najviše obraćala pažnju prilikom donošenja odluka
def generate_grad_cam(model, img_array, last_conv_layer_name):
    # Pravim novi, privremeni model koji ima dva izlaza: izlaz poslednjeg konvolucionog sloja i  finalni izlaz celog modela
    grad_model = Model(inputs=model.inputs, outputs=[model.get_layer(last_conv_layer_name).output, model.output])
    # Računanje gradijenata
    with tf.GradientTape() as tape:
        # Propuštam EKG primer kroz "grad_model" da dobijem oba izlaza
        last_conv_layer_output, preds = grad_model(img_array)
        # Uzimam indeks klase za koju je model dao najveću verovatnoću
        pred_index = tf.argmax(preds[0])
        # Fokusiram se samo na izlazni neuron za predviđenu klasu
        class_channel = preds[:, pred_index]
    # Sad računam gradijent finalnog rezultata u odnosu na izlaz poslednjeg konvolucionog sloja
    grads = tape.gradient(class_channel, last_conv_layer_output)
    # Računam prosečnu vrednost gradijenta za svaki filter i to mi daje jedan broj po filteru koji predstavlja važnost tog filtera
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    # Uzimam izlaz poslednjeg konvolucionog sloja
    last_conv_layer_output = last_conv_layer_output[0]
    '''
        Ključna stvar: množim svaku mapu karakteristika sa važnošću njenog filtera
        Ako je filter bio važan, delovi slike koje je on aktivirao će biti pojačani, ako nije bio važan biće utišani
        Rezultat je linearna kombinacija mapa karakteristika, ponderisana njihovom važnošću za odluku
    '''
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    # Uklanjam suvišne dimenzije
    heatmap = tf.squeeze(heatmap)
    # Zadržavam samo pozitivne aktivacije, tj one delove koji su pozitivno doprineli odluci i zatim delim sa maksimalnom vrednošću da bih skalirao heatmap u opseg [0,1] što je pogodno za vizuelni prikaz
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

# FUNKCIJE ZA ISCRTAVANJE GRAFIKONA
# Funkcija za iscrtavanje Grad-CAM
def plot_grad_cam(signal, heatmap):
    # Kreiram figuru i, što je važno, čuvamm reference na oba pod-grafikona (ax1, ax2)
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 6))

    # Prvi pod-grafikon: EKG Signal
    ax1.plot(signal[:, 0], label='EKG Signal (Lead I)')
    ax1.set_title('Originalni signal i Grad-CAM mapa važnosti', fontsize = 16)
    ax1.legend()
    ax1.grid(True)

    # Drugi pod-grafikon: Mapa važnosti (Heatmap)
    ax2.imshow(heatmap.T, cmap='jet', aspect='auto', extent=[0, signal.shape[0], 0, 1])
    ax2.set_xlabel('Vreme (uzorci)')
    ax2.set_yticks([])
    ax2.set_title('Mapa važnosti (Heatmap)', fontsize = 16)

    # Poravnavanje X-osa
    # Eksplicitno postavljam granice X-ose za oba grafikona na isti opseg, od 0 do dužine signala, bez dodatnog praznog prostora.
    ax1.set_xlim(0, signal.shape[0])
    ax2.set_xlim(0, signal.shape[0])

    plt.tight_layout() # Pomoćna funkcija koja lepo "spakuje" grafikone
    plt.show()

# Funkcija za iscrtavanje K-fold rezultata
def plot_kfold_results(results):
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=pd.DataFrame(results))
    plt.title(f'Distribucija performansi kroz {K_FOLDS} fold-ova', fontsize = 16)
    plt.ylabel('Vrednost metrike')
    plt.grid(True)
    plt.show()
# Funkcija za poređenje signala pre i posle obrade
def plot_signal_comparison(original, filtered, sample_idx=0, lead_idx=0):
    plt.figure(figsize=(15, 5))
    plt.title(f'Poređenje signala pre i posle predobrade (Uzorak {sample_idx}, Lead {lead_idx})', fontsize = 16)
    plt.plot(original[sample_idx, :, lead_idx], label='Originalni signal', alpha=0.7)
    plt.plot(filtered[sample_idx, :, lead_idx], label='Obrađeni signal', alpha=0.9)
    plt.xlabel('Vreme (uzorci)', fontsize = 16)
    plt.ylabel('Amplituda', fontsize = 16)
    plt.legend()
    plt.grid(True); plt.show()
# Funkcija za dijagnostiku overfittinga
def plot_history(history):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))
    ax1.plot(history.history['accuracy'], label='Trening tačnost')
    ax1.plot(history.history['val_accuracy'], label='Validaciona tačnost')
    ax1.set_title('Tačnost modela', fontsize = 16)
    ax1.set_xlabel('Epoha')
    ax1.legend()
    ax2.plot(history.history['loss'], label='Trening gubitak')
    ax2.plot(history.history['val_loss'], label='Validacioni gubitak')
    ax2.set_title('Gubitak modela', fontsize = 16)
    ax2.set_xlabel('Epoha')
    ax2.legend()
    plt.show()
# Funkcija koja prikazuje ROC krivu i AUC skor koje su standard za evaluaciju binarnih klasifikatora
def plot_roc_curve(y_test, y_pred_probs, classes):
    plt.figure(figsize=(10, 8))
    fpr, tpr, _ = roc_curve(y_test[:, 1], y_pred_probs[:, 1])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC kriva (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Stopa lažno pozitivnih (False Positive Rate)')
    plt.ylabel('Stopa istinski pozitivnih (True Positive Rate)')
    plt.title('ROC Kriva za detekciju MI', fontsize = 16)
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()
# Funkcija koja prikazuje koliko je model zapravo dobar u razdvajanju klasa.
def plot_probability_distribution(y_pred_probs, y_test, classes):
    plt.figure(figsize=(12, 6))
    mi_group_probs = y_pred_probs[y_test[:, 1] == 1, 1]
    normal_group_probs = y_pred_probs[y_test[:, 0] == 1, 1]
    sns.histplot(mi_group_probs, color="red", label='Stvarni MI', kde=True, stat="density", alpha=0.6)
    sns.histplot(normal_group_probs, color="blue", label='Stvarni Normal', kde=True, stat="density", alpha=0.6)
    plt.title('Distribucija predviđenih verovatnoća za MI klasu', fontsize = 16)
    plt.xlabel('Verovatnoća predikcije da je klasa MI')
    plt.legend()
    plt.show()


if __name__ == '__main__':
    # Učitavam sirove podatke
    X_raw, y = load_data(SAMPLING_RATE, BASE_PATH)
    # Proveravam da li je učitavanje bilo uspešno
    if X_raw.shape[0] > 0:
        # Predobrada signala, filtriranje i standardizacija
        X_processed = preprocess_signals(X_raw, SAMPLING_RATE, FILTER_TYPE)
        # Pozivam funkciju da prikažem kako izgleda signal nakon predobrade
        plot_signal_comparison(X_raw, X_processed, sample_idx=10)

        # Priprema labela (ciljnih promenljivih)
        lb = LabelBinarizer();
        # Pretvaram tekstualne labele u brojeve
        y_one_hot = lb.fit_transform(y)
        '''
            Ukoliko imam 2 klase kao u mom slučaju, neuronske mreže očekuju klasifikaciju u formatu npr [0, 1] za MI i [1, 0] za Normal
            LabelBinarizer za 2 klase vraća samo jednu kolonu pa je proširujem na dve kolone
        '''
        if len(lb.classes_) == 2: y_one_hot = np.hstack((1 - y_one_hot, y_one_hot))

        # Dinamičko preoblikovanje podataka za 1D ili 2D CNN
        if MODEL_TYPE == '2D':
            # Ako koristim 2D CNN podaci moraju imati 4 dimenzije: broj uzoraka, visina, širina i boje kanala
            # Moj EKG snimak ima oblik (broj_uzoraka, vreme, EKG_kanali) pa reshape dodaje četvrtu dimenziju tretirajući EKG kao crno belu sliku
            X_reshaped = X_processed.reshape(X_processed.shape[0], X_processed.shape[1], X_processed.shape[2], 1)
        else: # Za 1D CNN
            # Ako koristim 1D CNN nije potrebna nikakva promena jer već imam potreban oblik
            X_reshaped = X_processed

        # Obrađujem oblik ulaza za neuronsku mrežu ignorišući prvu dimenziju (broj_uzoraka)
        input_shape = X_reshaped.shape[1:]

        # Kreiram rečnik modela koji mapira nazive modela sa funkcijama koje ih grade
        model_builders = {
            'simple_2d': build_simple_2d_cnn, 'advanced_2d': build_advanced_2d_cnn,
            'simple_1d': build_simple_1d_cnn, 'advanced_1d': build_advanced_1d_cnn
        }
        # Kreiram ključ koji odgovara mom ključu na osnovu parametara koje sam definisao na početku
        model_key = f"{SELECTED_MODEL}_{MODEL_TYPE.lower()}"
        # Koristim odgovarajuću funkciju za izgradnju modela iz rečnika
        model_builder = model_builders[model_key]

        # Ako koristim K-fold unakrsnu validaciju
        if USE_KFOLD:
            print(f"\n Početak {K_FOLDS}-Fold unakrsne validacije")
            '''
                StratifiedKFold je napredna verzija KFold-a, to znači da će
                prilikom podele podataka na K delova (fold-ova), u svakom delu biti
                sačuvan isti procentualni odnos klasa ('MI' i 'Normal') kao u originalnom setu
                Ovo je jako važno za balansirane (ili nebalansirane) skupove podataka
                n_splits=K_FOLDS broj delova na koje se dele podaci
                shuffle=True podaci se promešaju pre podele
                random_state=42 osigurava da je "nasumično" mešanje uvek isto, radi ponovljivosti
            '''
            kfold = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)
            # Rečnik za čuvanje rezultata iz svakog fold-a
            fold_results = {'accuracy': [], 'loss': [], 'auc': []}
            # Glavna petlja, split metoda vraća indekse za trening i test skup za svaku iteraciju, koristim y za stratifikaciju
            for fold, (train_idx, test_idx) in enumerate(kfold.split(X_reshaped, y)):
                print(f"\n Fold {fold+1}/{K_FOLDS}")
                # Kreiram trening i test skupove za trenutni fold koristeći dobijene indekse
                X_train, X_test = X_reshaped[train_idx], X_reshaped[test_idx]
                y_train, y_test = y_one_hot[train_idx], y_one_hot[test_idx]
                # U svakom foldu kreiram novi, netrenirani model i to je važno kako bih očuvao nezavisnost svake evaluacije
                model = model_builder(input_shape)
                callbacks = [EarlyStopping(patience=5, restore_best_weights=True), ReduceLROnPlateau(patience=3)]
                # Treniram model na trening podacima trenutnog folda, izdvajam 15% trening podataka za evaluaciju kako bi EarlyStopping mogao raditi
                history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.15, callbacks=callbacks, verbose=1)
                # Evaluacija modela na test podacima trenutnog folda
                loss, acc = model.evaluate(X_test, y_test, verbose=0)
                # Dobijam predviđene verovatnoće za testni skup
                y_pred_probs = model.predict(X_test)
                # Računam ROC krivu i AUC skor za ovaj fold
                fpr, tpr, _ = roc_curve(y_test[:, 1], y_pred_probs[:, 1])
                roc_auc = auc(fpr, tpr)
                # Čuvanje rezultata i ispisivanje rezultata za trenutni fold
                fold_results['accuracy'].append(acc)
                fold_results['loss'].append(loss)
                fold_results['auc'].append(roc_auc)
                print(f"Rezultati za Fold {fold+1}: Tačnost={acc:.4f}, Gubitak={loss:.4f}, AUC={roc_auc:.4f}")
            # Prikaz rezultata, ralunam prosek i standardnu devijaciju (manja devijacija znači stabilniji model) za svaku metriku
            print("\nFinalni rezultati unakrsne validacije")
            print(f"Prosečna tačnost: {np.mean(fold_results['accuracy']):.4f} (+/- {np.std(fold_results['accuracy']):.4f})")
            print(f"Prosečan AUC: {np.mean(fold_results['auc']):.4f} (+/- {np.std(fold_results['auc']):.4f})")
            # Prikazujem box plot
            plot_kfold_results(fold_results)

        else: # Standardni trening
            print("\nPočetak standardnog treninga")
            # Podela podataka na standardna tri dela, 70% za trening i po 15% za test i validaciju
            X_train, X_temp, y_train, y_temp = train_test_split(X_reshaped, y_one_hot, test_size=0.3, random_state=42, stratify=y_one_hot)
            X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)
            # Kreiram jednu instancu modela koristeći prethodno izabrani model
            model = model_builder(input_shape)
            # Pomoću ove funkcije ispisujem pregled arhitekture modela da bih video da li je model ispravno kreiran
            model.summary()
            callbacks = [EarlyStopping(patience=5, restore_best_weights=True), ReduceLROnPlateau(patience=3)]
            # Započinjem trening modela, govorim modelu da koristi prethodno definisani validacioni skup za praćenje performansi na kraju svake epohe
            history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val), callbacks=callbacks)
            # Nakon treninga iscrtavam grafike tačnosti i gubitka
            plot_history(history)
            # Ocena performansi na test skupu
            loss, acc = model.evaluate(X_test, y_test, verbose=0)
            print(f"\nRezultati na TEST skupu: Gubitak={loss:.4f}, Tačnost={acc:.4f}")
            # Određujem predviđene verovatnoće za svaki primer iz testnog skupa
            y_pred_probs = model.predict(X_test)
            # Konvertujem verovatnoće u finalne labele tako što uzimam indeks neurona sa najvećom vrednošću
            y_pred_labels = np.argmax(y_pred_probs, axis=1)
            # Radim isto i za stvarne labele da ih pripremim za poređenje
            y_test_labels = np.argmax(y_test, axis=1)
            # Ispisujem detaljan izveštaj sa metrikama za svaku klasu
            print("\nIzveštaj o klasifikaciji:\n", classification_report(y_test_labels, y_pred_labels, target_names=lb.classes_))
            # Kreiram i iscrtavam matricu konfuzije da vizuelno prikažem gde model greši
            cm = confusion_matrix(y_test_labels, y_pred_labels)
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=lb.classes_, yticklabels=lb.classes_)
            plt.title('Matrica konfuzije', fontsize = 16)
            plt.ylabel('Stvarna klasa')
            plt.xlabel('Predviđena klasa')
            plt.show()
            # Pozivam ostale funkcije za vizualizaciju
            plot_roc_curve(y_test, y_pred_probs, lb.classes_)
            plot_probability_distribution(y_pred_probs, y_test, lb.classes_)

        # Cilj ovog poslednjeg dela koda jeste da pokažem na osnovu čega je model doneo odluku, ukoliko sam koristo 2D CNN modele
        if MODEL_TYPE == '2D':
            print("\n Generisanje Grad-CAM vizualizacije")
            # Biramo jedan primer iz testnog skupa koji je stvarno MI
            mi_indices = np.where(np.argmax(y_test, axis=1) == 1)[0]
            # Proveravam da li sam pronašao neki MI primer u testnom skupu
            if len(mi_indices) > 0:
                # Ako jesam, uzimam indeks prvog pronađenog MI primera
                sample_idx = mi_indices[0]
                # Izvlačim EKG signal za taj primer
                sample_image = np.expand_dims(X_test[sample_idx], axis=0)

                # Proveravam da li je model ispravno klasifikovao primer
                prediction = np.argmax(model.predict(sample_image))
                # Uzimam stvarnu klasu za taj primer
                actual = np.argmax(y_test[sample_idx])

                print(f"Primer za Grad-CAM: Stvarna klasa={lb.classes_[actual]}, Predviđena klasa={lb.classes_[prediction]}")
                # Pozivam funkciju za vizualizaciju tako što joj prosleđujem originalni 2D signal i generisanu heatmapu
                heatmap = generate_grad_cam(model, sample_image, 'last_conv_layer')
                plot_grad_cam(X_test[sample_idx], heatmap)
            else:
                # Ako u tom mom malom testnom skupu nema MI primera poruka
                print("Nije pronađen nijedan MI primer u testnom setu za Grad-CAM.")